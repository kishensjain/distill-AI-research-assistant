# Distill ğŸ”

> Chat with anything. URLs, PDFs, Word docs, and YouTube videos â€” all in one place.

Distill is an AI-powered research assistant that lets you load multiple sources and have a conversation with them. Paste a URL, upload a document, or drop a YouTube link â€” then ask questions and get answers grounded in your content.

---

## Features

- ğŸŒ **URL ingestion** â€” paste any webpage URL and chat with its content
- ğŸ“„ **PDF & Word support** â€” upload `.pdf` and `.docx` files directly
- ğŸ¥ **YouTube transcripts** â€” paste a YouTube link and chat with the video
- ğŸ§  **Smart chunking** â€” content is split into chunks and only the most relevant ones are sent to the model
- âš¡ **Streaming responses** â€” answers stream in real time
- ğŸ“ **Auto summary** â€” sources are automatically summarised when loaded
- ğŸ’¬ **Multi-turn memory** â€” the assistant remembers your conversation
- ğŸ–¥ï¸ **Gradio web UI** â€” clean browser-based interface

---

## Tech Stack

- [Gradio](https://gradio.app) â€” web UI
- [OpenAI-compatible client](https://github.com/openai/openai-python) â€” LLM calls
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) â€” web scraping
- [pypdf](https://pypdf.readthedocs.io/) â€” PDF extraction
- [python-docx](https://python-docx.readthedocs.io/) â€” Word doc extraction
- [youtube-transcript-api](https://github.com/jdepoix/youtube-transcript-api) â€” YouTube transcripts
- [uv](https://github.com/astral-sh/uv) â€” package management

---

## Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/distill.git
cd distill
```

### 2. Install dependencies

```bash
uv sync
```

### 3. Set up your API key

Create a `.env` file in the root:

```
OLLAMA_API_KEY=your_key_here
```

Or if using a different provider, update the client in `src/ui.py` accordingly.

### 4. Run the app

```bash
uv run main.py
```

Then open [http://127.0.0.1:7860](http://127.0.0.1:7860) in your browser.

---

## Project Structure

```
distill/
â”œâ”€â”€ main.py          # Entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion.py # URL, file, and YouTube loading
â”‚   â”œâ”€â”€ chunker.py   # Text splitting and relevance scoring
â”‚   â””â”€â”€ ui.py        # Gradio interface and LLM chat logic
â”œâ”€â”€ .env             # API keys (not committed)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## How It Works

1. **Ingest** â€” content is fetched and cleaned from your sources
2. **Chunk** â€” content is split into overlapping chunks of ~1000 characters
3. **Retrieve** â€” when you ask a question, the most relevant chunks are selected using keyword matching
4. **Generate** â€” selected chunks are sent to the LLM along with your question and conversation history
5. **Stream** â€” the response streams back in real time

This is a lightweight implementation of **RAG (Retrieval-Augmented Generation)** built from scratch without any RAG framework.

---

## Roadmap

- [ ] Semantic search with embeddings
- [ ] Save and load chat sessions
- [ ] Deploy to Hugging Face Spaces
- [ ] Support for more file types (CSV, TXT, EPUB)

---

## License

MIT